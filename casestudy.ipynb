{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1417fd61",
   "metadata": {},
   "source": [
    "# Build a Custom OpenAI Chatbot with ML-Driven Prompt Engineering\n",
    "\n",
    "The code below is designed to run as-is with one exception: **the OpenAI API key must be specified**. Edit the cell below to add your API key between the double quotes.\n",
    "\n",
    "Then, to execute each code cell, click on it and press `Shift` + `Enter` on your keyboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e212665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_base = \"https://openai.vocareum.com/v1\"\n",
    "openai.api_key = \"YOUR API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45c16c",
   "metadata": {},
   "source": [
    "## Step 0: Inspecting Non-Customized Results\n",
    "\n",
    "Before we perform any prompt engineering, **let's ask the OpenAI model some questions and see how it answers**.\n",
    "\n",
    "(If you encounter an `AuthenticationError` when running this code, make sure that you have added a valid API key to the cell above and executed it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukraine_prompt = \"\"\"\n",
    "Question: \"When did Russia invade Ukraine?\"\n",
    "Answer:\n",
    "\"\"\"\n",
    "initial_ukraine_answer = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=ukraine_prompt,\n",
    "    max_tokens=150\n",
    ")[\"choices\"][0][\"text\"].strip()\n",
    "print(initial_ukraine_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f85c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_prompt = \"\"\"\n",
    "Question: \"Who owns Twitter?\"\n",
    "Answer:\n",
    "\"\"\"\n",
    "initial_twitter_answer = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=twitter_prompt,\n",
    "    max_tokens=150\n",
    ")[\"choices\"][0][\"text\"].strip()\n",
    "print(initial_twitter_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66923e33",
   "metadata": {},
   "source": [
    "The model is answering this way because the training data ends in 2021. **Our task will be to provide context from 2022 to help the model answer these questions correctly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee2b145",
   "metadata": {},
   "source": [
    "# Step 1: Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb8bb1",
   "metadata": {},
   "source": [
    "## Loading and Wrangling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d50cc7e",
   "metadata": {},
   "source": [
    "**The data should be loaded into a pandas `DataFrame` called `df` where each row represents a text sample, and there is only one column, `\"text\"`, which contains the raw text data.**\n",
    "\n",
    "In this particular case we are collecting data from [the Wikipedia page for the year 2022](https://en.wikipedia.org/wiki/2022) and performing some data wrangling to get it into the appropriate format. Don't worry too much about the details here, since data wrangling looks different for every dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f517cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Get the Wikipedia page for \"2022\" since OpenAI's models stop in 2021\n",
    "resp = requests.get(\"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&exlimit=1&titles=2022&explaintext=1&formatversion=2&format=json\")\n",
    "\n",
    "# Load page text into a dataframe\n",
    "df = pd.DataFrame()\n",
    "df[\"text\"] = resp.json()[\"query\"][\"pages\"][0][\"extract\"].split(\"\\n\")\n",
    "\n",
    "# Clean up text to remove empty lines and headings\n",
    "df = df[(df[\"text\"].str.len() > 0) & (~df[\"text\"].str.startswith(\"==\"))]\n",
    "\n",
    "# In some cases dates are used as headings instead of being part of the\n",
    "# text sample; adjust so dated text samples start with dates\n",
    "prefix = \"\"\n",
    "for (i, row) in df.iterrows():\n",
    "    # If the row already has \" - \", it already has the needed date prefix\n",
    "    if \" – \" not in row[\"text\"]:\n",
    "        try:\n",
    "            # If the row's text is a date, set it as the new prefix\n",
    "            parse(row[\"text\"])\n",
    "            prefix = row[\"text\"]\n",
    "        except:\n",
    "            # If the row's text isn't a date, add the prefix\n",
    "            row[\"text\"] = prefix + \" – \" + row[\"text\"]\n",
    "df = df[df[\"text\"].str.contains(\" – \")]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c88a55",
   "metadata": {},
   "source": [
    "## Generating Embeddings\n",
    "\n",
    "We'll use the `Embedding` tooling from OpenAI [documentation here](https://platform.openai.com/docs/guides/embeddings/embeddings) to create vectors representing each row of our custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91eeef",
   "metadata": {},
   "source": [
    "In order to avoid a `RateLimitError` we'll send our data in batches to the `Embedding.create` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
    "batch_size = 100\n",
    "embeddings = []\n",
    "for i in range(0, len(df), batch_size):\n",
    "    # Send text data to OpenAI model to get embeddings\n",
    "    response = openai.Embedding.create(\n",
    "        input=df.iloc[i:i+batch_size][\"text\"].tolist(),\n",
    "        engine=EMBEDDING_MODEL_NAME\n",
    "    )\n",
    "    \n",
    "    # Add embeddings to list\n",
    "    embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n",
    "\n",
    "# Add embeddings list to dataframe\n",
    "df[\"embeddings\"] = embeddings\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c66e9",
   "metadata": {},
   "source": [
    "In order to avoid having to run that code again in the future, we'll save the generated embeddings as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f4909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdddaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6362dc3",
   "metadata": {},
   "source": [
    "If you want to stop the tutorial here and come back, you can reload `df` using this code (again adding your API key) rather than generating the embeddings again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09921453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import openai\n",
    "# openai.api_base = \"https://openai.vocareum.com/v1\"\n",
    "# openai.api_key = \"YOUR API KEY\"\n",
    "# df = pd.read_csv(\"embeddings.csv\", index_col=0)\n",
    "# df[\"embeddings\"] = df[\"embeddings\"].apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d9df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d3f08",
   "metadata": {},
   "source": [
    "# Step 2: Create a Function that Finds Related Pieces of Text for a Given Question\n",
    "\n",
    "What we are implementing here is similar to a search engine or recommendation algorithm. We want to sort all of the rows of our dataset from least relevant to most relevant.\n",
    "\n",
    "This will use the embeddings that we generated previously in order to compare the vectorized version of our question to the vectorized versions of the rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import get_embedding, distances_from_embeddings\n",
    "\n",
    "def get_rows_sorted_by_relevance(question, df):\n",
    "    \"\"\"\n",
    "    Function that takes in a question string and a dataframe containing\n",
    "    rows of text and associated embeddings, and returns that dataframe\n",
    "    sorted from least to most relevant for that question\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get embeddings for the question text\n",
    "    question_embeddings = get_embedding(question, engine=EMBEDDING_MODEL_NAME)\n",
    "    \n",
    "    # Make a copy of the dataframe and add a \"distances\" column containing\n",
    "    # the cosine distances between each row's embeddings and the\n",
    "    # embeddings of the question\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"distances\"] = distances_from_embeddings(\n",
    "        question_embeddings,\n",
    "        df_copy[\"embeddings\"].values,\n",
    "        distance_metric=\"cosine\"\n",
    "    )\n",
    "    \n",
    "    # Sort the copied dataframe by the distances and return it\n",
    "    # (shorter distance = more relevant so we sort in ascending order)\n",
    "    df_copy.sort_values(\"distances\", ascending=True, inplace=True)\n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46ca43c",
   "metadata": {},
   "source": [
    "Let's test that out for a couple different questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b62af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rows_sorted_by_relevance(\"When did Russia invade Ukraine?\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a3b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rows_sorted_by_relevance(\"Who owns Twitter?\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7468993",
   "metadata": {},
   "source": [
    "# Step 3: Create a Function that Composes a Text Prompt\n",
    "\n",
    "Building on that sorted list of rows, we're going to select the create a text prompt that provides context to a `Completion` model in order to help it answer a question. The outline of the prompt looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab2a827",
   "metadata": {},
   "source": [
    "```\n",
    "Answer the question based on the context below, and if the\n",
    "question can't be answered based on the context, say \"I don't\n",
    "know\"\n",
    "\n",
    "Context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74ff9d",
   "metadata": {},
   "source": [
    "We want to fit as much of our dataset as possible into the \"context\" part of the prompt without exceeding the number of tokens allowed by the `Completion` model, which is currently 4,000. So we'll loop over the dataset, counting the tokens as we go, and stop when we hit the limit. Then we'll join that list of text data into a single string and add it to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c237429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def create_prompt(question, df, max_token_count):\n",
    "    \"\"\"\n",
    "    Given a question and a dataframe containing rows of text and their\n",
    "    embeddings, return a text prompt to send to a Completion model\n",
    "    \"\"\"\n",
    "    # Create a tokenizer that is designed to align with our embeddings\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Count the number of tokens in the prompt template and question\n",
    "    prompt_template = \"\"\"\n",
    "Answer the question based on the context below, and if the question\n",
    "can't be answered based on the context, say \"I don't know\"\n",
    "\n",
    "Context: \n",
    "\n",
    "{}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    current_token_count = len(tokenizer.encode(prompt_template)) + \\\n",
    "                            len(tokenizer.encode(question))\n",
    "    \n",
    "    context = []\n",
    "    for text in get_rows_sorted_by_relevance(question, df)[\"text\"].values:\n",
    "        \n",
    "        # Increase the counter based on the number of tokens in this row\n",
    "        text_token_count = len(tokenizer.encode(text))\n",
    "        current_token_count += text_token_count\n",
    "        \n",
    "        # Add the row of text to the list if we haven't exceeded the max\n",
    "        if current_token_count <= max_token_count:\n",
    "            context.append(text)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return prompt_template.format(\"\\n\\n###\\n\\n\".join(context), question)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2c6a02",
   "metadata": {},
   "source": [
    "Now let's test that out! We'll use a `max_token_count` below the actual limit just to keep the output shorter and more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_prompt(\"When did Russia invade Ukraine?\", df, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b6fb2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(create_prompt(\"Who owns Twitter?\", df, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f11b5",
   "metadata": {},
   "source": [
    "# Step 4: Create a Function that Answers a Question\n",
    "\n",
    "Our final step is to send that text prompt to a `Completion` model and parse the model output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETION_MODEL_NAME = \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "def answer_question(\n",
    "    question, df, max_prompt_tokens=1800, max_answer_tokens=150\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a question, a dataframe containing rows of text, and a maximum\n",
    "    number of desired tokens in the prompt and response, return the\n",
    "    answer to the question according to an OpenAI Completion model\n",
    "    \n",
    "    If the model produces an error, return an empty string\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = create_prompt(question, df, max_prompt_tokens)\n",
    "    \n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=COMPLETION_MODEL_NAME,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_answer_tokens\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6598a361",
   "metadata": {},
   "source": [
    "Now that we have all of the code complete, let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16adca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ukraine_answer = answer_question(\"When did Russia invade Ukraine?\", df)\n",
    "print(custom_ukraine_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71822dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_twitter_answer = answer_question(\"Who owns Twitter?\", df)\n",
    "print(custom_twitter_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad6fb6",
   "metadata": {},
   "source": [
    "Below we compare answers with and without our custom prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8577558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "When did Russia invade Ukraine?\n",
    "\n",
    "Original Answer: {initial_ukraine_answer}\n",
    "Custom Answer:   {custom_ukraine_answer}\n",
    "\n",
    "Who owns Twitter?\n",
    "Original Answer: {initial_twitter_answer}\n",
    "Custom Answer:   {custom_twitter_answer}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecaede2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You just used unsupervised machine learning to perform prompt engineering for custom OpenAI chat responses!\n",
    "\n",
    "In this example, we provided context from 2022 news headlines to answer questions about current events. Try finding your own dataset for some other custom task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16421d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
